{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Data Mining\n",
    "Ngoc-Bien NGUYEN et Xian YANG, février 2018   \n",
    "Répertoire Github : https://github.com/MarchesLearning/salaryDiscovery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction du rapport\n",
    "\n",
    "Le rapport résume une analyse sur un jeu de données issu d'un sondage aux Etats-Unis de 42.882 sujets. Le but de notre projet data mining est d'étudier et d'appliquer les méthodes non supervisées, tout en se familarisant avec la procédure entière du traitement de données. Le rapport est organisé comme suit.  \n",
    "     \n",
    "\n",
    "Dans la section 1 on introduit le jeu de données et discute certaines techniques de feature selection. Dans la section 2 on fouille les données de façon à trouver les formes plus intéressantes en utilisant l'exceptional model mining. Dans la section 3 on cherche à réduire la dimension du jeu de données afin de le visualiser. Dans la section 4 on voit de différentes méthodes de clustering. dans la section 5 on étudie de façon détaillée la recherche de motifs fréquents et de règles d'association avec notre jeu de données.  \n",
    "\n",
    "Au contraire de notre cours rapport, le code dans les fichiers `Jupyter` est bien rangé et commenté et ces fichiers sont enrichis de plein de graphiques. Nous vous conseillons _fortement_ de jeter un coup d'oeil dans ces 4 fichiers `Jupyter` qui ne sont pas longs du tout, au moins sur les graphes là-bas, qui ont été plottés avec coeur. Manque de temps pour la mise en page on n'a pas transmis les graphes dans ce rapport. Donc veuillez retrouver là-bas les éléments les plus parlants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et pré-traitement du jeu de données\n",
    "\n",
    "Le jeu de données qu'un analyste a dans ses mains, tout comme celui de notre projet, est souvent rempli de données brutes. Il peut y avoir des attributs quantitatifs, qualitatifs ou encore ordinaux. Il peut comtenir des valeurs manquantes. Il peut y avoir des informations redondantes ou inutiles. Ainsi, il convient, avant tout d'autres traitements et de mises en place des algorithmes, de bien observer le jeu de données et d'ajuster les variables pour qu'elles soient sous formes pertinantes pour les prochaines étapes. En effet, la bonne compréhension er la bonne représentation du jeu de données jouent un rôle beaucoup plus important que beaucoup le pensent.\n",
    "\n",
    "### 1.1 Première constatation du jeu de données et ajustement des variables (feature selection)\n",
    "Nous avons pour notre projet un jeu de données appelé \"adult income dataset\" issu d'un certain bureau de recensement gouvernemental américain. Ce jeu de données a relevé, pour 48.842 sujets de l'enquête apparamment tous résidant aux Etats-Unis, leur situation familiale, parcours académique, profession, âge, race, sex, pays d'origine, temps de travail hebdomadaire, fortune personnelle et, ce qui est le plus important plus tard pour la partie Machine Learning, _leur salaire_. Lorsque ce dernier est au coeur de la prédiction là-bas, ici on le considère comme toutes les autres variables. Mais toutefois, dans l'ensemble de notre analyse on mettra quand-même l'accent sur la relation entre le salaire et les autres variables puisqu'il est visiblement le but de recensement original.  \n",
    "  \n",
    "On regarde maintenant les variables de près.   \n",
    "  \n",
    "Parmi les 15 variables 6 sont quantitatives : âge, numérotation du parcours académique, fortune personnelle en gain, fortune personnelle en perte, temps de travail hebdomadaire et une certaine \"fnlwgt\". Les 9 autres sont qualitatives : secteur d'embauche, parcours académique, état civil, situation familiale, profession, race, sexe, pays d'origine et le salaire. Il est à remarquer que le salaire n'est pas numérique : en revanche, il est juste indiqué si le salaire est supérieur ou inférieur à 50k dollars par an.   \n",
    "  \n",
    "Qu'est-ce qu'on peut encore faire à part cette observation ? On doit regarder si les variables sont présentées de façon pertinente qui faciliteront notre analyse. Le jeu de données nous permet d'illustrer cas perticuliers. \n",
    "\n",
    "#### La variable parait merveilleuse mais on ne sait pas de quoi elle parle. Ce genre de variable doit être supprimée et on ne doit pas faire de l'analyse à l'aveugle avec elle.\n",
    "La variable \"fnlwgt\" correspond apparament à une certaine classification de dossier au bureau de recensement. De toute façon sans information précise on ne sait pas l'interpreter. On la supprime donc sans hésitation. \n",
    "\n",
    "#### Les variables semblent de parler des différents aspects d'un même sujet et de pouvoir être fusionnées. Qu'on les fusionne ou pas, nécessite une analyse cas par cas.\n",
    "La fortune en gain et la fortune en perte sont des variables opossantes : un sujet possède au plus une valeur non-nulle dans ces deux variables. On pourrait réfléchir que celles-là représentent en effet une même information et qu'on pourrait les mettre ensemble. Mais pourtant, vu qu'une fortune négative puisse avoir un sens particulier : les gens qui n'ont pas eu de succès dans leurs investissements ne sont pas forcément des pauvres mais en revanche souvent des riches, on croit que le signe de la fortune n'est pas un signe pûrement mathématique. De ce fait, on préfère garder les deux variables telles quelles. \n",
    "\n",
    "#### Les variables sont redondantes et on doit en garder une seule. \n",
    "La numérotation du parcours académique est juste un codage numérique de la variable qualitative (ordinale) parcours académique. Allant de 1 à 16, 16 représente le professorat, 15 la thèse, 14 le diplôme de master et ainsi de suite jusqu'à ce que 1 représente un à 4 ans d'école primaire. La variable qualitative doit être enlevée, puisque la garde des deux amène des informations redondantes qui vont biaser le jeu de données. En plus, la raison pour laquelle on préfère garder la variable quantitative est simplement que cette variable est de nature ordinale. Il convient de prendre la forme qui peut révéler ce sens d'ordre.\n",
    "\n",
    "#### La variable est numérique mais elle n'est peut-être pas linéairement proportionnelle à son vrai sens. Une transformation peut-être considérée.\n",
    "Toujours à la variable numérotation du parcours académique, on peut y avoir deux réflexions. Premièrement, les informations sont trop détaillées de manière qu'un numéro plus grand ne représente pas forcément un diplôme avec plus de valeur, bien que la tendance générale soit bonne. Par exemple, _Assoc-voc_, _Assoc-acdm_ et _Some-college_ sont codées respectivement en 11, 12 et 13. On ne sait pas trop ce que c'est aux états-unis et si ces situations-là délivrent vraiment des regards différents chez les employeurs (on pense toujours à la relation entre la variable en question et le _salaire_). On veut donc les regrouper. Pareil, tout ceux qui n'ont même pas réussi leur collège n'ont pas de grande différence au marché d'emploi, indépendemment du nombre d'années qu'ils ont passé à l'école. On va donc les confondre aussi. Deuxièmement, cette numérotation n'est proportionnelle ni à la nombre d'années d'études qu'il faut pour obtenir le diplôme, ni à la valeur du diplôme que les gens pensent en général. Par exemple, le doctorat est codé par 15 et le master par 14, alors que le bachelor est codé par 13 et le Bac par 9. Souvent une thèse est beaucoup plus cherchée qu'un diplôme de master. Donc on va donner à cette variable plutôt une échelle exceptionnelle de telle façon que le doctorat ou le professorat vaut 1, le master que la moitié, le bachelor qu'un quart et ainsi de suite.\n",
    "\n",
    "#### La variable est qualitative et elle est trop détaillée. On doit penser à regrouper certaines valeurs. \n",
    "C'est le cas de la variable pays d'origine. Elle a une trentaine d'étiquettes. Vu que l'enquête a probablement été faite aux Etats-Unis et que 9/10 des sujets sont de nationalité américaine, on peut penser à remplacer cette variable pas deux : 1. le sujet est américain ou non, 2. le sujet est ressortissant d'un pays développé ou non. Ces deux variables permettent de mettre en évidence cette information.\n",
    "\n",
    "### 1.2 Traitement des données manquantes\n",
    "Le traitement des données manquantes est souvent une étape de pré-traitement qu'il faut bien soigner. Heureusement, le jeu de données de notre projet ne possède que 7% de valeurs manquantes sur un nombre d'observations de 14.882. Donc même supprimer toutes ces observations peut être une option. En effet, c'est les trois variables qualitatives : secteur d'embauche, profession et pays d'origine qui contiennent des valeurs manquantes. Ainsi, les méthodes de remplacer une valeur manquante par la moyenne ou encore une valeur spécifique n'ont pas d'application ici. On croit que le fait que les gens n'ont pas renseigné d'information sur leur emploi ou leur pays d'origine peut avoir un sens particulier. Voire il se peut que le sujet sans ce renseignement est en chômage (cette catégorie n'existe par exemple pas dans la variable profession). On va donc créer pour chacune des trois variables une nouvelle catégorie composée des valeurs manquantes.\n",
    "\n",
    "### 1.3 Statistique descriptive et résumé du jeu de données\n",
    "Il est toujours utile de faire une statistique descriptive sur chaque variable (si pas trop nombreuses) pour avoir un aperçu du jeu de données avant tout algorithme compliqué. Par exemple, dans notre exemple on peut voir que 9/10 des sujets sont des américains et parmi les étrangers les méxicains sont le plus nombreux ; la plupart des sujets travaille dans le secteur privé ; il y a trois fois autant de sujets dont le salaire ne dépasse pas 50K qu'il y a de sujets dont le salaire dépasse ce seuil mais en revanche, ce taux est inversé parmi les gens qui ont le titre de doctorat ; il y a deux fois d'hommes qu'il y a de femmes etc.  \n",
    "  \n",
    "On peut aussi étudier les relations entre variables. Dans ce cas-là, une matrice de corrélation de toutes les variables est conseillée dans un premier plan (voir `Exploration.ipynb`). On y voit des informations un peu moins évidentes : bien qu'en nombre total il y a plus d'hommes que de femmes, dans le sous-groupes jamais-mariés il y a beaucoup plus de femmes que d'hommes.\n",
    "\n",
    "### 1.4 Standardisation des variables quantitatives et binarisation des variables qualitatives (one-hot transformation)\n",
    "Une fois le traitement de valeurs manquantes fixé et les variables ainsi que les modalités restantes considérées pertinentes, on peut transformer les variables qualitatives en colonnes par la représentation _one-hot_(binaire). Ici il y a deux cas de figures. Si dans une variable catégorique il y a des valeurs manquantes, on adopte l'approche qui transforme toutes les autres modalités chacune en une colonne et ne garde pas les valeurs manquantes. Mais en effet, l'information qu'une observation est en valeur manquante dans cette variable n'est pas perdue puisqu'elle a 0 partout dans les nouvelles colonnes produises par les modalités non-manquantes. Par contre si une variable catégorique ne contient aucune valeur manquante, on va ignorer la modalité la plus nombreuse et transformer les restes, pour la même raison qu'avant. En tout cas, il vaut mieux d'éviter les informations redondantes.   \n",
    "  \n",
    "D'ailleurs, pour les variables numériques, puisqu'elles ont toujours leur propre échelle, on les standardise dans l'intervalle \\[0, 1\\]. \n",
    "  \n",
    "\n",
    "Finalement lorsque les données sont prêtes, on peut passer à notre analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exceptional Model Mining\n",
    "L'exceptional model mining est une famille d'approches qui sert à détecter des sous-groupes _qui n'ont pas forcément une valeur abberante selon un critère simple, mais plutôt se comportent de façon différente du reste du groupe_. Nous avons utilisé les algorithmes proposés par [cet article](https://github.com/MarchesLearning/IdeesBrouillonCommunications/blob/master/ExceptionalModelMining.pdf), qui contient de riches discussions théoriques qu'on va laisser ici.   \n",
    "  \n",
    "Pour illustrer cette merveilleuse méthode avec notre jeu de données, on se pose deux questions intéressantes.  \n",
    "\n",
    "    1. Par de simples analyses statistiques on peut donner à chacun des pays d'origine étudiés une note qui représente le niveau d'études moyen de ses ressortissants aux Etats-Unis (classement : taïwanais à la place 1, suivis par les indiens et les chinois) et, une autre qui représente la proportion des salariés au-dessus de 50K parmi ses ressortissants aux Etats-Unis (classement : français à la première place, suivis par les indiens et les taïwanais). On peut voit que ces deux classement ne sont pas le même. Or quel est le lien entre eux et comment le quentifier ? Quelle interprétation peut se cacher derière ce lien ?\n",
    "    \n",
    "    2. Lorsqu'on attend en général une augmentation salariale avec celle de son âge d'un salarié, ce n'est pas toujours le cas avec le nombre d'heure de travail hebdomadaire. A-ce dernier un lien avec l'âge ?\n",
    "    \n",
    "On essaie de répondre à ces deux questions avec l'EMM. Pour la première question, on construit pour chaque pays un modèle de régression logistique uni-varié qui prédit la classe de salaire de ses ressortissants avec leurs niveaux d'études\n",
    "$$ P(salaire >= 50K \\mid formation) = \\frac{1}{1 + e^{-(\\beta * formation + \\beta_0)}}.$$\n",
    "Lorsque le taux de bonnes prédictions dépasse 70%, on considère que le modèle est fiable pour ce pays d'origine. Une fois qu'on obtient tous les modèles de RL ajustés, on peut faire un nouveau classement de la pente $\\beta$ de la fonction de lien. Plus $\\beta$ est grande, mieux le diplôme d'études est recompensé par le salaire. On peut argumenter que cette pente réflète d'une certaine manière l'égalité sociale. Le résultat n'est pas étonnant : le système américain est \"le plus juste\" pour les américains, c'est-à-dire que les américains sont en moyenne les plus motivés à faire des études parce qu'il est le plus rentable. Ce n'est pas le cas chez les pays asiatiques qui ont les meilleurs scores tout à l'heure en termes de diplômés. La raison derrière pourrait être les contraîntes sur le marché du travail pour les étrangers.  \n",
    "  \n",
    "On pourrait aussi faire une étude pareil sur la rentabilité du niveau d'études au sein des américains mais avec les différentes éthniques.  \n",
    "  \n",
    "Pour la deuxième question, on regarde pour chaque profession la corrélation entre l'âge des salariés et leurs nombres d'heures de travail hebdomadaires de ce sous-groupe. Lorsque pour la plupart des professions il n'y a pas de forte corrélation, cette valeur est de 0,4 chez les forces de l'ordre et de -0,1 chez les gardes. Cela pourrait être expliqué par le fait que l'expérience qui se cumule avec l'augmentation de l'âge est importante pour les forces de l'ordre, tant dis que les conditions physiques jouent un rôle essentiel pour les gardes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Méthodes de réduction de dimensions : t-SNE et Auto-encodeur\n",
    "Les méthodes de réduction de dimensions peuvent servir non seulement comme un pré-traitement avant le clustering, mais elles sont aussi un outil essentiel pour visualiser les données en 2 dimensions.  \n",
    "  \n",
    "La méthode de réduction de dimensions la plus courante en data mining est l'analyse en composantes principales. Néanmoins, après une analyse de variances expliquées on se rend vite compte que cette méthode n'est pas pertinente pour la visualisation de notre jeu de données, puisque les deux premières composantes principales n'expliquent ensemble même pas 30% de la variance totale. Il faut donc en chercher d'autres.  \n",
    "  \n",
    "Dans notre projet on a essayé deux nouvelles approches : le _t-distributed stochastic neighbor embedding (t-SNE)_ et l'_Auto-encodeur_, toutes deux étant des méthodes non-linéaires.  \n",
    "  \n",
    "### 3.1 t-distributed stochastic neighbor embedding\n",
    "Le t-SNE tente à garder les points qui sont originalement proches l'un de l'autre dans l'espace projeté. L'un de ses caractéristiques c'est que l'image d'arrivée fait toujours un _seul_ nuage de points. On illustre cette méthode avec notre jeu de données en les colorant selon de différents critères. Les graphes sont à voir dans le fichier _tsne_autoencoder.ipynb_ (les premiers six sub-plots).   \n",
    "  \n",
    "On voit que les deux classes de salaires se dégagent bien dans la présentation graphique, ce qui indique que la variable salaire est une forme importante de ce jeu de données, même si elle n'occupe qu'une seule colonne. Les étrangers sont aussi à reconnaître dans le graphe en se regroupant dans le centre du disque. En revanche, le sexe, la profession, l'état civil et la formation ne montrent pas de forme évidente, au moins pas dans le graphe de t-SNE, ce qui nous amène à vouloir essayer une autre méthode de réduction de dimensions, l'auto-encodeur.  \n",
    "  \n",
    "### 3.2 Auto-encodeur\n",
    "L'auto-encodeur est une méthode de réseau de neuronnes composé d'un encodeur et d'un décodeur, où le dernier a exactement la même structure que le premier juste dans le sens opposé. L'essentiel de cette méthode consiste à encoder les entrées dans un espace de dimensions réduites (souvent 2 pour but de visualisation) en cherchant à perdre le minimum d'informations possible. Elle ne pose aucune métrique de distance au préalable. Ici on illustre également cette méthode avec notre jeu de données en les colorant selon les mêmes critères qu'avant. Les graphes sont à voir dans le fichier _tsne_autoencoder.ipynb_ (les deuxièmes six sub-plots).   \n",
    "  \n",
    "Cette fois-ci ce ne sont plus un seul nuage de points mais les individus se regroupent quasiment parfaitement en environ 7 sous-groupes. En plus, la coloration nous permet de bien identifier les salariés élevés, le sexe et les américains. Il est aussi à remarquer que sur le dernier graphe de formation, les points jaunes (donc les meilleurs diplômés) se trouvent pour la plupart dans la région des salariés dépassant 50K, lorsque sur le graphe d'état civil en bas au milieu on voit que les jamais-mariés se trouvent principalement dans la région des salariés moins de 50K (ce qui est logique, pour la raison d'âge) et qu'en plus, ce sont notamment des femmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering\n",
    "Dans cette section, on utilise le t-SNE et l'encodeur de la méthode auto-encodeur pour visualiser les différentes approches de clustering.  \n",
    "  \n",
    "Les méthodes de clustering servent à aider les analystes de détecter des formes dans les données. dans notre projet, on part plutôt du but de retrouver dans les clusters les sous-groupes originaux obtenus par différentes variables. On se concentre sur 6 méthodes de clustering de base : K-Means, Hiérarchique, BDSCAN, Mean Shift, Affinity Propagation et Spéctral. On voit que pas toutes ces méthodes de clustering sont compatibles avec notre jeu de données et que même au sein d'une même méthode de clustering, les différents paramètres de départ ou d'autres détails peuvent mener à des résultats très différents.   \n",
    "   \n",
    "Pour voir les graphes de clustering en 2D, dirigez-vous vers `Clustering.ipynb`.  \n",
    "   \n",
    "On commence par __K-Means__. On met par hasard le nombre de clusters à 5. On voit que le résultat présenté par l'auto-encodeur est assez bon : il a retrouvé les salariés > 50K ; les femmes dans le sous-groupe salaire <= 50K ainsi que d'autres points qui se regroupent de façon naturelle.  \n",
    "  \n",
    "Ensuite on essaie le clustering __hiérarchique__ (agglomératif). On peut faire plusieurs choses avec cette méthode. En premier lieu, on fait un double-clustering par rapport aux observatiosn et par rapport aux variables.\n",
    "Puis on compare la performance des trois linkages : \"ward\", \"average\" et \"complete\", en mettant par hasard le nombre de clusters à 6. On voit que \"ward\" et \"complete\" fonctionne aussez bien.   \n",
    "  \n",
    "On s'intéresse aussi à __DBSCAN__. Cette méthode est basée sur la densité des nuages de points. Ainsi, il peut subir du fléau de la dimension dans notre exemple puisque dans la haute-dimension les points sont creux partout.  \n",
    "  \n",
    "Finalement le clustering __spéctral__, basé sur la théorie des graphes, peut révéler quelques formes, alors que l'__affinity propagation___ et le __mean shift__ n'a pas révélé d'information très intéressante, pour une raison similaire que _DBSCAN_. Toutefois, on peut bien imginer que DBSCAN va fonctionner parfaitement si on diminue la dimension avec l'auto-encodeur avant de donner les données à cet algorithme de clustering.\n",
    "\n",
    "Pour plus de détails, veuillez lire le fichier `Clustering.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recherche de motifs fréquents et de règles d'association\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
