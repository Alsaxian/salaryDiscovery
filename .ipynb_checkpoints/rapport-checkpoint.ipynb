{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction du rapport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et pré-traitement du jeu de données\n",
    "\n",
    "Le jeu de données qu'un analyste a dans ses mains, tout comme celui de notre projet, est souvent rempli de données brutes. Il peut y avoir des attributs quantitatifs, qualitatifs ou encore ordinaux. Il peut comtenir des valeurs manquantes. Il peut y avoir des informations redondantes ou inutiles. Ainsi, il convient, avant tout d'autres traitements et de mises en place des algorithmes, de bien observer le jeu de données et d'ajuster les variables pour qu'elles soient sous formes pertinantes pour les prochaines étapes. En effet, la bonne compréhension er la bonne représentation du jeu de données jouent un rôle beaucoup plus important que beaucoup le pensent.\n",
    "\n",
    "### 1.1 Première constatation du jeu de données et ajustement des variables (feature selection)\n",
    "Nous avons pour notre projet un jeu de données appelé \"adult income dataset\" issu d'un certain bureau de recensement gouvernemental américain. Ce jeu de données a relevé, pour 48.842 sujets de l'enquête apparamment tous résidant aux Etats-Unis, leur situation familiale, parcours académique, profession, âge, race, sex, pays d'origine, temps de travail hebdomadaire, fortune personnelle et, ce qui est le plus important plus tard pour la partie Machine Learning, _leur salaire_. Lorsque ce dernier est au coeur de la prédiction là-bas, ici on le considère comme toutes les autres variables. Mais toutefois, dans l'ensemble de notre analyse on mettra quand-même l'accent sur la relation entre le salaire et les autres variables puisqu'il est visiblement le but de recensement original.  \n",
    "  \n",
    "On regarde maintenant les variables de près.   \n",
    "  \n",
    "Parmi les 15 variables 6 sont quantitatives : âge, numérotation du parcours académique, fortune personnelle en gain, fortune personnelle en perte, temps de travail hebdomadaire et une certaine \"fnlwgt\". Les 9 autres sont qualitatives : secteur d'embauche, parcours académique, état civil, situation familiale, profession, race, sexe, pays d'origine et le salaire. Il est à remarquer que le salaire n'est pas numérique : en revanche, il est juste indiqué si le salaire est supérieur ou inférieur à 50k dollars par an.   \n",
    "  \n",
    "Qu'est-ce qu'on peut encore faire à part cette observation ? On doit regarder si les variables sont présentées de façon pertinente qui faciliteront notre analyse. Le jeu de données nous permet d'illustrer cas perticuliers. \n",
    "\n",
    "#### La variable parait merveilleuse mais on ne sait pas de quoi elle parle. Ce genre de variable doit être supprimée et on ne doit pas faire de l'analyse à l'aveugle avec elle.\n",
    "La variable \"fnlwgt\" correspond apparament à une certaine classification de dossier au bureau de recensement. De toute façon sans information précise on ne sait pas l'interpreter. On la supprime donc sans hésitation. \n",
    "\n",
    "#### Les variables semblent de parler des différents aspects d'un même sujet et de pouvoir être fusionnées. Qu'on les fusionne ou pas, nécessite une analyse cas par cas.\n",
    "La fortune en gain et la fortune en perte sont des variables opossantes : un sujet possède au plus une valeur non-nulle dans ces deux variables. On pourrait réfléchir que celles-là représentent en effet une même information et qu'on pourrait les mettre ensemble. Mais pourtant, vu qu'une fortune négative puisse avoir un sens particulier : les gens qui n'ont pas eu de succès dans leurs investissements ne sont pas forcément des pauvres mais en revanche souvent des riches, on croit que le signe de la fortune n'est pas un signe pûrement mathématique. De ce fait, on préfère garder les deux variables telles quelles. \n",
    "\n",
    "#### Les variables sont redondantes et on doit en garder une seule. \n",
    "La numérotation du parcours académique est juste un codage numérique de la variable qualitative (ordinale) parcours académique. Allant de 1 à 16, 16 représente le professorat, 15 la thèse, 14 le diplôme de master et ainsi de suite jusqu'à ce que 1 représente un à 4 ans d'école primaire. La variable qualitative doit être enlevée, puisque la garde des deux amène des informations redondantes qui vont biaser le jeu de données. En plus, la raison pour laquelle on préfère garder la variable quantitative est simplement que cette variable est de nature ordinale. Il convient de prendre la forme qui peut révéler ce sens d'ordre.\n",
    "\n",
    "#### La variable est numérique mais elle n'est peut-être pas linéairement proportionnelle à son vrai sens. Une transformation peut-être considérée.\n",
    "Toujours à la variable numérotation du parcours académique, on peut y avoir deux réflexions. Premièrement, les informations sont trop détaillées de manière qu'un numéro plus grand ne représente pas forcément un diplôme avec plus de valeur, bien que la tendance générale soit bonne. Par exemple, _Assoc-voc_, _Assoc-acdm_ et _Some-college_ sont codées respectivement en 11, 12 et 13. On ne sait pas trop ce que c'est aux états-unis et si ces situations-là délivrent vraiment des regards différents chez les employeurs (on pense toujours à la relation entre la variable en question et le _salaire_). On veut donc les regrouper. Pareil, tout ceux qui n'ont même pas réussi leur collège n'ont pas de grande différence au marché d'emploi, indépendemment du nombre d'années qu'ils ont passé à l'école. On va donc les confondre aussi. Deuxièmement, cette numérotation n'est proportionnelle ni à la nombre d'années d'études qu'il faut pour obtenir le diplôme, ni à la valeur du diplôme que les gens pensent en général. Par exemple, le doctorat est codé par 15 et le master par 14, alors que le bachelor est codé par 13 et le Bac par 9. Souvent une thèse est beaucoup plus cherchée qu'un diplôme de master. Donc on va donner à cette variable plutôt une échelle exceptionnelle de telle façon que le doctorat ou le professorat vaut 1, le master que la moitié, le bachelor qu'un quart et ainsi de suite.\n",
    "\n",
    "#### La variable est qualitative et elle est trop détaillée. On doit penser à regrouper certaines valeurs. \n",
    "C'est le cas de la variable pays d'origine. Elle a une trentaine d'étiquettes. Vu que l'enquête a probablement été faite aux Etats-Unis et que 9/10 des sujets sont de nationalité américaine, on peut penser à remplacer cette variable pas deux : 1. le sujet est américain ou non, 2. le sujet est ressortissant d'un pays développé ou non. Ces deux variables permettent de mettre en évidence cette information.\n",
    "\n",
    "### 1.2 Traitement des données manquantes\n",
    "Le traitement des données manquantes est souvent une étape de pré-traitement qu'il faut bien soigner. Heureusement, le jeu de données de notre projet ne possède que 7% de valeurs manquantes sur un nombre d'observations de 14.882. Donc même supprimer toutes ces observations peut être une option. En effet, c'est les trois variables qualitatives : secteur d'embauche, profession et pays d'origine qui contiennent des valeurs manquantes. Ainsi, les méthodes de remplacer une valeur manquante par la moyenne ou encore une valeur spécifique n'ont pas d'application ici. On croit que le fait que les gens n'ont pas renseigné d'information sur leur emploi ou leur pays d'origine peut avoir un sens particulier. Voire il se peut que le sujet sans ce renseignement est en chômage (cette catégorie n'existe par exemple pas dans la variable profession). On va donc créer pour chacune des trois variables une nouvelle catégorie composée des valeurs manquantes.\n",
    "\n",
    "### 1.3 Statistique descriptive et résumé du jeu de données\n",
    "dire juste les proportions des étiquettes dans les variables qualitatives, s'il y a des particularités etc.\n",
    "\n",
    "### 1.4 Standardisation des variables quantitatives et binarisation des variables qualitatives (one-hot transformation)\n",
    "Finalement lorsque les données sont prêtes, on peut commencer notre analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exceptional Model Mining\n",
    "L'exceptional model mining est une famille d'approches qui sert à détecter des sous-groupes _qui n'ont pas forcément une valeur abberante selon un critère simple, mais plutôt se comportent de façon différente du reste du groupe_. Nous avons utilisé les algorithmes proposés par [cet article](https://github.com/MarchesLearning/IdeesBrouillonCommunications/blob/master/ExceptionalModelMining.pdf), qui contient de riches discussions théoriques qu'on va laisser ici.   \n",
    "  \n",
    "Pour illustrer cette merveilleuse méthode avec notre jeu de données, on se pose deux questions intéressantes.  \n",
    "\n",
    "    1. Par de simples analyses statistiques on peut donner à chacun des pays d'origine étudiés une note qui représente le niveau d'études moyen de ses ressortissants aux Etats-Unis (classement : taïwanais à la place 1, suivis par les indiens et les chinois) et, une autre qui représente la proportion des salariés au-dessus de 50K parmi ses ressortissants aux Etats-Unis (classement : français à la première place, suivis par les indiens et les taïwanais). On peut voit que ces deux classement ne sont pas le même. Or quel est le lien entre eux et comment le quentifier ? Quelle interprétation peut se cacher derière ce lien ?\n",
    "    \n",
    "    2. Lorsqu'on attend en général une augmentation salariale avec celle de son âge d'un salarié, ce n'est pas toujours le cas avec le nombre d'heure de travail hebdomadaire. A-ce dernier un lien avec l'âge ?\n",
    "    \n",
    "On essaie de répondre à ces deux questions avec l'EMM. Pour la première question, on construit pour chaque pays un modèle de régression logistique uni-varié qui prédit la classe de salaire de ses ressortissants avec leurs niveaux d'études\n",
    "$$ P(salaire >= 50K \\mid formation) = \\frac{1}{1 + e^{-(\\beta * formation + \\beta_0)}}.$$\n",
    "Lorsque le taux de bonnes prédictions dépasse 70%, on considère que le modèle est fiable pour ce pays d'origine. Une fois qu'on obtient tous les modèles de RL ajustés, on peut faire un nouveau classement de la pente $\\beta$ de la fonction de lien. Plus $\\beta$ est grande, mieux le diplôme d'études est recompensé par le salaire. On peut argumenter que cette pente réflète d'une certaine manière l'égalité sociale. Le résultat n'est pas étonnant : le système américain est \"le plus juste\" pour les américains, c'est-à-dire que les américains sont en moyenne les plus motivés à faire des études parce qu'il est le plus rentable. Ce n'est pas le cas chez les pays asiatiques qui ont les meilleurs scores tout à l'heure en termes de diplômés. La raison derrière pourrait être les contraîntes sur le marché du travail pour les étrangers.  \n",
    "  \n",
    "On pourrait aussi faire une étude pareil sur la rentabilité du niveau d'études au sein des américains mais avec les différentes éthniques.  \n",
    "  \n",
    "Pour la deuxième question, on regarde pour chaque profession la corrélation entre l'âge des salariés et leurs nombres d'heures de travail hebdomadaires de ce sous-groupe. Lorsque pour la plupart des professions il n'y a pas de forte corrélation, cette valeur est de 0,4 chez les forces de l'ordre et de -0,1 chez les gardes. Cela pourrait être expliqué par le fait que l'expérience qui se cumule avec l'augmentation de l'âge est importante pour les forces de l'ordre, tant dis que les conditions physiques jouent un rôle essentiel pour les gardes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Méthodes de réduction de dimensions : t-SNE et Auto-encodeur\n",
    "Les méthodes de réduction de dimensions peuvent servir non seulement comme un pré-traitement avant le clustering, mais elles sont aussi un outil essentiel pour visualiser les données en 2 dimensions.  \n",
    "  \n",
    "La méthode de réduction de dimensions la plus courante en data mining est l'analyse en composantes principales. Néanmoins, après une analyse de variances expliquées on se rend vite compte que cette méthode n'est pas pertinente pour la visualisation de notre jeu de données, puisque les deux premières composantes principales n'expliquent ensemble même pas 30% de la variance totale. Il faut donc en chercher d'autres.  \n",
    "  \n",
    "Dans notre projet on a essayé deux nouvelles approches : le _t-distributed stochastic neighbor embedding (t-SNE)_ et l'_Auto-encodeur_, toutes deux étant des méthodes non-linéaires.  \n",
    "  \n",
    "Le t-SNE tente à garder les points qui sont originalement proches l'un de l'autre dans l'espace projeté. L'un de ses caractéristiques c'est que l'image d'arrivée fait toujours un _seul_ nuage de points. On illustre cette méthode avec notre jeu de données en les colorant selon de différents critères. Les graphes sont à voir dans le fichier _tsne_autoencoder.ipynb_ (les premiers six sub-plots). On voit que les deux classes de salaires se dégagent bien dans la présentation graphique, ce qui indique que la variable salaire est une forme importante de ce jeu de données, même si elle n'occupe qu'une seule colonne. Les étrangers sont aussi à reconnaître dans le graphe en se regroupant dans le centre du disque. En revanche, le sexe, la profession, l'état civil et la formation ne montrent pas de forme évidente, au moins pas dans le graphe de t-SNE, ce qui nous amène à vouloir essayer une autre méthode de réduction de dimensions, l'auto-encodeur.  \n",
    "  \n",
    "L'auto-encodeur est une méthode de réseau de neuronnes composé d'un encodeur et d'un décodeur, où le dernier a exactement la même structure que le premier juste dans le sens opposé. L'essentiel de cette méthode consiste à encoder les entrées dans un espace de dimensions réduites (souvent 2 pour but de visualisation) en cherchant à perdre le minimum d'informations possible. Elle ne pose aucune métrique de distance au préalable. Ici on illustre également cette méthode avec notre jeu de données en les colorant selon les mêmes critères qu'avant. Les graphes sont à voir dans le fichier _tsne_autoencoder.ipynb_ (les deuxièmes six sub-plots). Cette fois-ci ce ne sont plus un seul nuage de points mais les individus se regroupent quasiment parfaitement en environ 7 sous-groupes. En plus, a coloration nous permet de bien identifier les salariés élevés, le sexe et les américains. Il est aussi à remarquer que sur le dernier graphe de formation, les points jaunes (donc les meilleurs diplômés) se trouvent pour la plupart dans la région des salariés dépassant 50K, lorsque sur le graphe d'état civil en bas au milieu on voit que les jamais-mariés se trouvent principalement dans la région des salariés moins de 50K (ce qui est logique, pour la raison d'âge) et qu'en plus, ce sont notamment des femmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
